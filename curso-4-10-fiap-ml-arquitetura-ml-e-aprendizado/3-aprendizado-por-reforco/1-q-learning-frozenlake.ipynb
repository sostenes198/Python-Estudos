{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Neste notebook, voc√™ codificar√° do zero seu primeiro agente de Reinforcement Learning jogando FrozenLake ‚ùÑÔ∏è usando Q-Learning\n",
   "id": "150ac1e3bd92be9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Adaptado HuggingFace",
   "id": "6ec081efdd5fa954"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://www.gymlibrary.dev/_images/frozen_lake.gif\" alt=\"Environments\"/>",
   "id": "10ba5a35997dfd6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###üéÆ Environments:\n",
    "\n",
    ">\n",
    "\n",
    "- [FrozenLake-v1](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)\n",
    "\n",
    "\n",
    "###üìö RL-Library:\n",
    "\n",
    "- Python and NumPy\n",
    "- [Gym](https://www.gymlibrary.dev/)"
   ],
   "id": "3b02a88bab89a3a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pequena revis√£o de Q-Learning",
   "id": "b5fe3ddae27a9a0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- O *Q-Learning* **√© o algoritmo RL que**\n",
    "\n",
    "   - Treina *Q-Function*, uma **fun√ß√£o a√ß√£o-valor (action-value function)** que cont√©m, como mem√≥ria interna, uma *Q-table* **que cont√©m todos os valores do par estado-a√ß√£o.**\n",
    "\n",
    "   - Dado um estado e uma a√ß√£o, nossa Q-Function **pesquisar√° em sua Q-table o valor correspondente.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n",
    "\n",
    "- Quando o treinamento √© conclu√≠do,**temos uma Fun√ß√£o-Q ideal, portanto, uma Tabela-Q ideal.**\n",
    "\n",
    "- E se **tivermos uma fun√ß√£o Q √≥tima**,\n",
    "ter uma pol√≠tica ideal, pois **sabemos para cada estado qual √© a melhor a√ß√£o a ser tomada.**\n",
    "\n",
    "Mas, no come√ßo, nossa **Q-Table √© in√∫til, pois fornece um valor arbitr√°rio para cada par estado-a√ß√£o (na maioria das vezes, inicializamos a Q-Table com valores 0)**. Mas, conforme vamos explorando o ambiente e atualizando nosso Q-Table, ele nos dar√° aproxima√ß√µes cada vez melhores\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
    "\n",
    "This is the Q-Learning pseudocode:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ],
   "id": "e9ff1fce88473182"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's code our first Reinforcement Learning algorithm üöÄ",
   "id": "8100a8f37d76dec2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:43:01.027808800Z",
     "start_time": "2026-02-21T22:43:00.593368100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# O modo 'human' abre uma janela nativa no Windows\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.reset()"
   ],
   "id": "81f74bf4dbe32cba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02533983,  0.04777611,  0.00611492, -0.00021378], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:40:16.280268Z",
     "start_time": "2026-02-21T23:40:16.270378700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import os\n",
    "import tqdm\n",
    "import imageio\n",
    "\n",
    "import pickle as pickle\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm"
   ],
   "id": "3567d671dc18194f",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:49:47.651862800Z",
     "start_time": "2026-02-21T22:49:47.629830Z"
    }
   },
   "cell_type": "code",
   "source": "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)",
   "id": "1fe64fbd782f5411",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:54:12.133539800Z",
     "start_time": "2026-02-21T22:54:12.104790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ],
   "id": "28c739165e7c95e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(16)\n",
      "Sample observation 2\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T22:54:35.905134700Z",
     "start_time": "2026-02-21T22:54:35.876733100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ],
   "id": "6d5e332455bfdeb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 3\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:08:02.374555800Z",
     "start_time": "2026-02-21T23:08:02.348913100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ],
   "id": "afa68278027bc4c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  16  possible states\n",
      "There are  4  possible actions\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:09:20.933637Z",
     "start_time": "2026-02-21T23:09:20.920282300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vamos criar nossa Qtable de tamanho (state_space, action_space) e inicializar cada valor em 0 usando np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable"
   ],
   "id": "b9c69ec87e6a04cc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:30:53.203605600Z",
     "start_time": "2026-02-21T23:30:53.179179400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "\n",
    "Qtable_frozenlake"
   ],
   "id": "9edb70df07f91c1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defina a pol√≠tica gananciosa ü§ñ\n",
    "Lembre-se de que temos duas pol√≠ticas, pois o Q-Learning √© um algoritmo **off-policy**. Isso significa que estamos usando uma **pol√≠tica diferente para atuar e atualizar a fun√ß√£o de valor**.\n",
    "\n",
    "- Pol√≠tica Epsilon-gananciosa (pol√≠tica de atua√ß√£o)\n",
    "- Greedy-policy (pol√≠tica de atualiza√ß√£o)\n",
    "\n",
    "A pol√≠tica gananciosa tamb√©m ser√° a pol√≠tica final que teremos quando o agente Q-learning for treinado. A pol√≠tica gulosa √© usada para selecionar uma a√ß√£o da tabela Q.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ],
   "id": "98add62970bbda1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:19:22.218514400Z",
     "start_time": "2026-02-21T23:19:22.203687300Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 15,
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "\n",
    "  return action\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Gera aleatoriamente um n√∫mero entre 0 e 1\n",
    "  random_int = random.uniform(0,1)\n",
    "  # if random_int > maior que epsilon --> exploitation\n",
    "  if random_int > epsilon:\n",
    "     # Execute a a√ß√£o com o maior valor dado um estado\n",
    "     # np.argmax pode ser √∫til aqui\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "  return action"
   ],
   "id": "4c067a87cf97afeb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definindo os hiperpar√¢metros ‚öôÔ∏è\n",
    "Os hiperpar√¢metros relacionados √† explora√ß√£o s√£o alguns dos mais importantes.\n",
    "\n",
    "- Precisamos garantir que nosso agente **explore o espa√ßo de estados** o suficiente para aprender uma boa aproxima√ß√£o de valor. Para fazer isso, precisamos ter decaimento progressivo do epsilon.\n",
    "- Se voc√™ diminuir o epsilon muito r√°pido (decay_rate muito alto), **voc√™ corre o risco de que seu agente fique preso**, j√° que seu agente n√£o explorou o espa√ßo de estado o suficiente e, portanto, n√£o pode resolver o problema."
   ],
   "id": "88c1642d9978c374"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:30:46.586799700Z",
     "start_time": "2026-02-21T23:30:46.578370100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Par√¢metros de treinamento\n",
    "n_training_episodes = 10000 # Total de epis√≥dios de treinamento\n",
    "learning_rate = 0.7 # Taxa de aprendizado\n",
    "\n",
    "# Par√¢metros de avalia√ß√£o\n",
    "n_eval_episodes = 100 # N√∫mero total de epis√≥dios de teste\n",
    "\n",
    "# Par√¢metros do ambiente\n",
    "env_id = \"FrozenLake-v1\" # Nome do ambiente\n",
    "max_steps = 99 # Max passos por epis√≥dio\n",
    "gamma = 0.95 # Taxa de desconto\n",
    "eval_seed = [] # A semente de avalia√ß√£o do ambiente\n",
    "\n",
    "# Par√¢metros de explora√ß√£o\n",
    "max_epsilon = 1.0 # Probabilidade de explora√ß√£o no in√≠cio\n",
    "min_epsilon = 0.05 # Probabilidade m√≠nima de explora√ß√£o\n",
    "decay_rate = 0.0005 # Taxa de decaimento exponencial para prob de explora√ß√£o"
   ],
   "id": "74c3b7b03a1a2f4d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Rotina de Treinamento\n",
    "\n",
    "O loop de treinamento √© assim:\n",
    "```\n",
    "Por epis√≥dio no total de epis√≥dios de treino:\n",
    "\n",
    "Reduza o epsilon (j√° que precisamos cada vez menos de explora√ß√£o)\n",
    "Redefinir o ambiente\n",
    "\n",
    "   Para passo em passos de tempo m√°ximo:\n",
    "     Escolha a a√ß√£o At usar a pol√≠tica gananciosa do epsilon\n",
    "     Tome a a√ß√£o (a) e observe o(s) estado(s) resultante(s) e a recompensa (r)\n",
    "     Atualize o valor Q Q(s,a) usando a equa√ß√£o de Bellman Q(s,a) + lr [R(s,a) + gama * max Q(s',a') - Q(s,a)]\n",
    "     Se terminar, termine o epis√≥dio\n",
    "     Nosso pr√≥ximo estado √© o novo estado\n",
    "```"
   ],
   "id": "5f9ce16d7d64041a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:35:06.391008100Z",
     "start_time": "2026-02-21T23:35:06.375539900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduzir epsilon (exploration vs exploitation)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "\n",
    "    # 1. AJUSTE AQUI: O reset no Gymnasium retorna 'state' e 'info'\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # repete para cada passo dentro do epis√≥dio\n",
    "    for step in range(max_steps):\n",
    "      # Escolha a a√ß√£o usando a epsilon greedy policy\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "      # 2. Executa a a√ß√£o no ambiente (j√° estava certo com 5 vari√°veis)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      # 3. AJUSTE AQUI: Atualiza a vari√°vel 'done' combinando terminated e truncated\n",
    "      done = terminated or truncated\n",
    "\n",
    "      # Update Q(s,a) := Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      # (Garantindo que learning_rate e gamma estejam definidos globalmente no seu notebook)\n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "      # Verifica se o epis√≥dio acabou (caiu no buraco, pegou o frisbee ou estourou o limite de tempo)\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "      # Atualiza o estado atual para o pr√≥ximo passo\n",
    "      state = new_state\n",
    "\n",
    "  return Qtable"
   ],
   "id": "b9db509637daaf7e",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Treinando o agente Q-Learning üèÉ",
   "id": "ab1e0a6090841ae7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:35:09.215544900Z",
     "start_time": "2026-02-21T23:35:08.220584800Z"
    }
   },
   "cell_type": "code",
   "source": "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)",
   "id": "aec429a5fd2ce536",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|‚ñä         | 871/10000 [00:00<00:01, 8696.41it/s]\u001B[A\n",
      " 18%|‚ñà‚ñä        | 1838/10000 [00:00<00:00, 9267.99it/s]\u001B[A\n",
      " 29%|‚ñà‚ñà‚ñâ       | 2925/10000 [00:00<00:00, 9997.64it/s]\u001B[A\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 4041/10000 [00:00<00:00, 10443.25it/s]\u001B[A\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 5227/10000 [00:00<00:00, 10887.02it/s]\u001B[A\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 6316/10000 [00:00<00:00, 10886.06it/s]\u001B[A\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 7405/10000 [00:00<00:00, 10610.77it/s]\u001B[A\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 8575/10000 [00:00<00:00, 10933.24it/s]\u001B[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 10256.99it/s][A\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:35:38.273001500Z",
     "start_time": "2026-02-21T23:35:38.256884900Z"
    }
   },
   "cell_type": "code",
   "source": "Qtable_frozenlake",
   "id": "2736cbdeecd0690f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Avalia√ß√£o do M√©todo üìù\n",
    "\n",
    "- Definimos o m√©todo de avalia√ß√£o que vamos usar para testar nosso agente Q-Learning."
   ],
   "id": "7814680687b25259"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:38:37.626485Z",
     "start_time": "2026-02-21T23:38:37.609652700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "  \"\"\"\n",
    "   Avalie o agente para epis√≥dios ``n_eval_episodes`` e retorne recompensa m√©dia e desvio padr√£o de recompensa.\n",
    "   :param env: O ambiente de avalia√ß√£o\n",
    "   :param n_eval_episodes: N√∫mero de epis√≥dios para avaliar o agente\n",
    "   :param Q: A tabela Q\n",
    "   :param seed: A matriz de sementes de avalia√ß√£o (para taxi-v3)\n",
    "   \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "    # 1. AJUSTE AQUI: Desempacotar 'state' e 'info' do reset, com ou sem seed\n",
    "    if seed:\n",
    "      state, info = env.reset(seed=seed[episode])\n",
    "    else:\n",
    "      state, info = env.reset()\n",
    "\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      # Tome a a√ß√£o (√≠ndice) que tem a recompensa futura m√°xima esperada dado aquele estado\n",
    "      action = greedy_policy(Q, state)\n",
    "\n",
    "      # 2. AJUSTE AQUI: Receber as 5 vari√°veis do Gymnasium\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      # 3. AJUSTE AQUI: Atualizar o status de 'done'\n",
    "      done = terminated or truncated\n",
    "\n",
    "      total_rewards_ep += reward\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "      state = new_state\n",
    "\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ],
   "id": "20073ea6f6adbdfc",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Avaliando nosso agente Q-Learning üìà\n",
    "\n",
    "- Normalmente, voc√™ deve ter uma recompensa m√©dia de 1,0\n",
    "- O **ambiente √© relativamente f√°cil** j√° que o espa√ßo de estados √© muito pequeno (16). O que voc√™ pode tentar fazer √© [substitu√≠-lo pela vers√£o escorregadia](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/), que introduz estocasticidade, tornando o ambiente mais complexo."
   ],
   "id": "c85675f0ad466082"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:38:40.416814300Z",
     "start_time": "2026-02-21T23:38:40.351155900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate our Agent\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "id": "bdf6c877383a38de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 29595.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=1.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:42:49.681153300Z",
     "start_time": "2026-02-21T23:42:49.666966500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def record_video(env, Qtable, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "   Gerar um v√≠deo de replay do agente\n",
    "   :param env: O ambiente criado com render_mode='rgb_array'\n",
    "   :param Qtable: Qtable do nosso agente\n",
    "   :param out_directory: Caminho para salvar o v√≠deo (ex: 'replay.mp4')\n",
    "   :param fps: quantos quadros por segundo (com taxi-v3 e frozenlake-v1 usamos 1)\n",
    "   \"\"\"\n",
    "  images = []\n",
    "  done = False\n",
    "\n",
    "  # 1. AJUSTE: Desempacotar state e info no reset\n",
    "  state, info = env.reset(seed=random.randint(0,500))\n",
    "\n",
    "  # 2. AJUSTE: O Gymnasium moderno n√£o usa mais mode='' aqui.\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "\n",
    "  while not done:\n",
    "    # Tome a a√ß√£o (√≠ndice) que tem a recompensa futura m√°xima esperada dado aquele estado\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "\n",
    "    # 3. AJUSTE: Receber as 5 vari√°veis do step\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "\n",
    "  # Salvar o v√≠deo\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ],
   "id": "1f79e5a73a75a232",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T23:42:52.590055600Z",
     "start_time": "2026-02-21T23:42:51.714137400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Crie um NOVO ambiente com o render_mode=\"rgb_array\"\n",
    "# (Aten√ß√£o: ajuste o map_name e is_slippery para ficarem iguais ao do seu treino)\n",
    "env_video = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# 2. Defina o nome do arquivo\n",
    "video_path = \"replay.mp4\"\n",
    "\n",
    "# 3. Grave o v√≠deo passando o novo ambiente (env_video)\n",
    "# Dica: Use fps=1. Valores quebrados como 0.5 podem dar erro em alguns geradores de v√≠deo do Windows.\n",
    "record_video(env_video, Qtable_frozenlake, video_path, fps=1)\n",
    "\n",
    "# 4. Feche o ambiente ap√≥s gravar para liberar a mem√≥ria\n",
    "env_video.close()\n",
    "\n",
    "print(\"V√≠deo gravado com sucesso!\")"
   ],
   "id": "ea4f612bb6a74d02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V√≠deo gravado com sucesso!\n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
